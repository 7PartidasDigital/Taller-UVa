---
title: "R para los de Letras: Trabajando con textos"
author: "J. M. Fradejas"
date: "2024.12.19"
output: html_notebook
---
<div>
<center>
<img src="https://fyl.uva.es/wp-content/uploads/2020/07/fyluva-logo.png" width="350"/>
</center>
</div>


# Introduction

En el cuaderno anterior aprendiste a instalar R, RStudio y las librerías que utilizaremos en durante este taller. También te explique que siempre que veas una caja gris con código dentro de ella (como la que hay a continuación), debes seleccionar el contenido, cortarlo y pegarlo en el editor de RStudio y que debía ejecutarlo bien con la combinación `CTRL/CMD+`⏎ (`CTRL` en Windows; `CMD` en Mac) o haciendo clic en el icono Run que hay en la parte superior derecha de la ventana del editor.

Vamosa comenzar estableciendo los parámetros básicos de esta sesión

```{r, message=F, warning=F}
# Establecer las opciones
options(stringsAsFactors = F)                           
options(scipen = 999) 
options(max.print=100) 
```

y cargando las librería que usaremos.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(quanteda)
library(stopwords)
```


# Trabajar con textos{-}

Aunque R se creó para el análisis estadístico y la visualización de los resultados de esos análisis, también es útil para manejar datos textuales que podemos convertir en datos estadísticos y visualizarlos.


## Cargar datos textuales{-}

Un corpus clásico para enseñar los principios de la manipulación de textos con R es el _State of the Union Address_ (SOTU), un informe anual que, desde 1790, presenta el Presidente de los Estados Unidos en un sesión conjunta del Congreso. Lo vamos a utilizar, aunque también podríamos hacer uso de los mensajes de Navidad que pronuncia el Jefe del Estado en de Nochebuena, e incluso los debates del Estado de la Nación o los Debates de Investidura, o cualquier otro conjunto (corpus) de datos que se quiera explorar.

All todos los SOTU se pueden conseguir en la red, hay varias fuentes que los han publicado. Sin embargo, los he cosechado y he hecho una pequeña labor de limpieza y están disponibles en un repositorio de GitHub. Para cargarlos se puede utilizar la función `read_Lines` cuyo primer argumento es el nombre del fichero que contiene el texto (en este caso es una dirección —URL— de internet). Minemos duranto un ratito el discurso de 2022.

Lo primero de todo es cargarlo en un **objeto** que llamaremos `last_sotu`.

```{r, message=FALSE, warning=FALSE}
last_sotu <- read_lines(url("https://raw.githubusercontent.com/7PartidasDigital/MLex/master/sotu/242.txt"))

# Inspeccionemos el texto (solo verás la primera línea.)

str(last_sotu)
```

Una vez que lo hemos guardado en un **objeto**, es muy sencillo extraer información acerca de la frecuencia de las palabras que contiene y crear listas de palabras. Para lograrlo lo primero que hay que hacer es utilizar la función `unnest_tokens`, que dividirá el texto en palabras (tokens) y después usaremos la funcición `count` para conocer las frwecuencias absolutas de cada palabra tipo.

Lo primero que hay que hacer es transformar ese extenso texto en una especie de tabla con la función `tibble`. Esto hará que podamos manejarlo con mayor facilidad.

```{r, message=FALSE, warning=FALSE}
tidy_sotu <- tibble(text = last_sotu)

# Inspeccionamos los datos (solo las 10 primeras entradas)

tidy_sotu
```

<div class="warning" style='padding:0.1em; background-color:#1e8449; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>ATENCIÓN</b><br>Los nombres de los **objetos** los voy a utilizar en inglés, pero nada impide que uses palabras de la lengua que mejor que convenga. Solo hay unas pequeñas limitaciones: no pueden empezar por un número; distingue entre mayúsculas y minúsculas —no es lo mismo `Ciudad` que `ciudad` que `CIUDAD` son tres nombres nombre diferentes— y hay una serie de caracteres que no se pueden utilizar (los símbolos matemáticos, las barras, y algunos otros).<br>Y una recomendación: no uses letras con tildes, pueden ser fuente de tremendos quebraderos de cabeza.<br></p>
<p style='margin-left:1em;'>
</p></span>
</div>
<br>

Cada línea de esta tabla corresponde a un párrafo del discurso, es decir, incluye todo el texto que hay antes de cada golpe de la tecla ⏎. Ahora lo podemos dividir en tokens.

Siempre que veas una tabla con la de antes, puedes recorrerla haciendo clic en el `Next` que hay en la parte inferior de la tabla. 

```{r, message=FALSE, warning=FALSE}
tidy_words <- tidy_sotu %>%
  unnest_tokens(word, text) %>%
  count(word, sort=T)

# Inspeccionamos los datos (solo las 10 primeras entradas)

tidy_words
```

Lo que le henmos dicho a R que divida el texto que hay en el **objeto** `tidy_sotu` en palabras (tokens), que las cuente, las ordene de mayor a menor frecuencia (`n`) y las guarden en otro object llamado `tidy_words`.

Fíjate que en el margen inferior, en el lado izquierdo dice `1-10 of 1.735 rows`. Esto quiere decir que el texto del discurso de 2022 tiene 1735 palabras diferentes (palabras tipo), mientras que la columna `n` indica cuentas veces aparece cada una de las palabras tipo.

Tan fácil como dividir el texto en palabras individuales, es el extraer los N-grams, es decir, secuencias de n palabras consecutivas. Es sencillo porque la función `unnest_tokens` tiene un argumento llamado `token` por medio del cual le puedes indicar a R que quieres extraer `ngrams`. Si es lo que te interesa, entonces debes especificar el número de palabras que que ha de agrupar; para ello se utiliza el argumento `n`. Con el código que hay a continuación le pides a R que quieres extraer todos los grupos de cuatro palabras (4-grams) y que informe de la frecuencia absoluta y que las presente en orden decreciente. 

```{r, message=FALSE, warning=FALSE}
tidy_sotu %>%
  unnest_tokens(word,
                text,
                token = "ngrams",
                n = 4) %>%
  count(word, sort=T)
```

También es posible devidir el texto en oraciones (no enredemos con los coneptos de oración, aquí funcionan de otra manera) en vez de en palabras. De nuevo es fácil de hacer porque el argumento `tokens` de la función `unnest_tokens` puede tener el valor `sentences`.

```{r message=FALSE, warning=FALSE}
tidy_sotu %>%
  unnest_tokens(sentences,
                text,
                token = "sentences")
```

<div class="warning" style='padding:0.1em; background-color:#1e8449; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>ATENCIÓN</b><br>Cuando se le pide que divida cualquier texto en palabras, n-grams u oraciones, una de las cosas que hace es convertir las mayúsculas en minúsculas de manera que _First_ en _First Lady_ y la de cualquier caso de _first_ que pueda aparecer a lo largo del texto las cuente como la misma palabra tipo. Sé que esto es problemático en algunos casos, pero hay herramientas para solucionarlo.<br></p>
<p style='margin-left:1em;'>
</p></span>
</div>
<br>

Ya hemos visto que en el discurso de Biden de 2022 había 1735 palabras diferentes (palabras tipo), y vimos que cada una de ella aparece un número determinados de veces (_the_ aparece 300 veces, _and_ 264 y _to_ 234), pero no sabes cuántas son las palabras que conforman el discurso. Hay `6553` palabras token. la forma de calcularso es sumando todos los valores de `n` del objeto `tidy_words` y se consigue con la función `sum`.

```{r, message=FALSE, warning=FALSE}
sum(tidy_words$n)
```


## Stopwords{-}

Al dividir el texto en tokens, has visto que palabras corrientes como “the”, “to”, “and”, “of” y “we” están en lo alto de la tabla. estas palabras poco nos pueden decir del contenido del discurso (en otros tipos de análisis estas palabras son oro molido). Vas a borrarlas.

```{r, message=FALSE, warning=FALSE}
data(stop_words)
tidy_words %>%
  anti_join(stop_words)
```

A pesar de haber elminado todas las _palabras de función_ o _palabras vacías_ hay, sin embargo, algunas palabras muy corrientes y contracciones como “let's”, “that's”, “it's” o “we're”. Quizá te interese localizar todas aquellas palabras que se usan con muchísima más frecuencia en este texto de lo que se utilizan en un basto corpus del inglés. para conseguirlo, necesitas un dataset que recopile estas frecuencias. Uno muy bueno para eso es el de [Peter Norvig's](https://www.kaggle.com/datasets/rtatman/english-word-frequency) que se basa en el Google Web Trillion Word Corpus, que se ha contruido con textos extraidos de sitio web en inglés.

```{r, message=FALSE, warning=FALSE}
word_frequencies <- read_csv("https://raw.githubusercontent.com/programminghistorian/jekyll/gh-pages/assets/basic-text-processing-in-r/word_frequency.csv")
head(word_frequencies)
```

La primera columna indica la lengua de que se trata (es siempe“en” de inglés), la segunda da la palabras y la tercera porcentaje de uso de cada palabra en el Trillion Word Corpus.

Para combinar todas estas frecuencias con la table en la que tienes con los datos del discurso de Biden de 2022, usarás la función `inner_join`. esta función toma dos tablas (o dos dataset) y los combina en una solo en virtud de las columnas que tengan el mismo nombre; en este caso la columna común es la llamada `word`.

```{r, message=FALSE, warning=FALSE}
tidy_words %>%
  inner_join(word_frequencies) %>%
  filter(frequency < 0.1)
```
Esta lista ya empieza a tener un aspecto mejor. Términos como “america”, “american(s)”, “year”, “people” y “world” han escalado hasta la parte superior de la tabla y ya podemos especular que aparecen con muchas frencuencia en discursos de políticos, como es el caso del SOTU, pero su ocurrencia es relativamente menor en otros dominios. Si establecieras un umbral mucho más bajo, digamos del 0.02, podrás ver un resumen mucha más interesante del discurso.

```{r, message=FALSE, warning=FALSE}
tidy_words %>%
  inner_join(word_frequencies) %>%
  filter(frequency < 0.02)
```

Si haces clic en el `Next` de la parte baja de la tabla anterior, verás que aparecen algunos términos interesantes como “putin”, “ukrainian”, “allies”, “pandemic”, “deficit” e “inflation”. Estos paren ser algunos de los temas del discurso.

Antes de meterte en agua más profundas, vas ver un breve resumen basado en las cinco palabras más frecuentes de este discurso. para conseguirlo, necesitas otra trabla con una serie de metadatos que te permitan identificar totalmente cada uno de los discursos. Esta tabla coniene el nombre del Presidente, año del discurso, años en los que estuvo en la Casa Blanca (algo realmente irrelevante), partido al que pertenece y tipo de discurso, pues pueden escritos o hablados. Vas a cargarlo en tu ordenador.

```{r, message=FALSE, warning=FALSE}
sotu_meta <- read_tsv("https://raw.githubusercontent.com/7PartidasDigital/MLex/master/sotu_meta.txt")
# Muestra el contenido (solo el principio)
sotu_meta
```

Ya es hora de resumirlo en una sola línea. Al final de la instrucción hay un `5`, lo cual implica que seleccionará las cinco palabras más utilizadas, se puede incrementar a la cantidad que quieras, pero una sumario de más de 10 paalbras será muy poco informativo).

```{r, message=FALSE, warning=FALSE, error = FALSE}
address_summary <- tidy_words %>%
  inner_join(word_frequencies) %>%
  filter(frequency < 0.02)
result <- c(sotu_meta$president[242],
            sotu_meta$year[242],
            address_summary$word[1:5])
paste(result, collapse = "; ")
```
Creo que se podía haber eliminado la palabra "tonight", no dice nada del tema, tan solo que lo leyó una tarde noche.

## Concordancias KWIC{-}

Crear unas concardancias o tablas de palabras clave en contexto _key-word-in-context_ (KWIC) es uno de los procedimientos mñás usuales cuando se analizan datos textuales. No tienes que esforzarte mucho para conseguirlo, hay varias funciones que lo harán con sencillez. Utilizará la función `kwic` de la librería `quanteda` para crear una concordancia KWIC, pero exige dividir el texto en tokens con la función `tokens`. 

```{r, message=FALSE, warning=FALSE}
kwic_multiple <- last_sotu %>%
  tokens() %>%
  kwic(pattern = "Ukraine",
       window = 3) %>%
  as.data.frame()
# Inspecciona el comienzo de la tabla
head(kwic_multiple)
```

# Analizar todos y cada uno de los Discursos del State of the Union desde 1790 hasta 2024

El primer paso para analizar todos el corpus de los discursos del State of the Union es hacer que R los lea todos a la vez. Esto implica utilizar, como has hecho antes, la función `readLines`, pero unirás todos los párrafos de cada uno de los discursos con la función`paste`, añadirás un sistema de identificación para saber qué discurso es cada uno de ellos. Todo eswte se tiene que hacer por medio de un bucle, porque quieres meter nada más y nada menos que 244 discursos en una única tabal (u objeto) llamada `all_sotu`.

As the files are in the web, the very first step is to declare the full url for every speech and an empty table (`all_sotu`) to hold all the texts. The instruction to create the file names is a two steps process. Firstly we will store the invariable part of the URL in `base_url`. An secondly, to create the file names, that are made up of a number between `1` and `243`, we will use the `sprintf` function, that allows a formula to add the numbers, with leading zeros, from `001` … `099`… `243`, to the file extension `.txt` and glue it the invariable part of the url. 

```{r, message=FALSE, warning=FALSE}
base_url <- "https://raw.githubusercontent.com/7PartidasDigital/MLex/master"
files <- sprintf("%s/sotu/%03d.txt", base_url, 1:243)
all_sotu <- NULL
```

You may be wondering how the urls have turned out. Let's us cheks the six first with the `head` function.

```{r, message=FALSE, warning=FALSE}
head(files)
```

and the last six with the and `tail` function.


```{r, message=FALSE, warning=FALSE}
tail(files)
```

Now we can read them all into a table (it will take a bit)

```{r, message=FALSE, warning=FALSE}
for (i in 1:length(files)) {
  sotu <- readLines(files[i])
  sotu <- paste(sotu, collapse = "\n")
  temporary <- tibble(speech = i,
                      text = sotu)
  all_sotu <- bind_rows(all_sotu, temporary)
}
```

And will attach to it the metadata stored in `sotu_meta`:

```{r, message=FALSE, warning=FALSE}
all_sotu <- full_join(all_sotu, sotu_meta, by="speech")
```


## Exploratory Analysis

Now we have all the elements we need to analyze all SOTU speeches. Let's begin by split them into tokens. The already seen `unnest_tokens` function will do it in secs.

```{r, message=FALSE, warning=FALSE}
all_words <- all_sotu %>%
  unnest_tokens(word, text)
all_words
```

He have at the tip of our fingers a corpus of a little over 2M tokens (exactly 2,009,343 tokens). From here you can posit several questions as Is there a temporal pattern to the length of addresses? How do the lengths of the past several administration’s speeches compare to those of FDR, Abraham Lincoln, and George Washington?

The best way to see this is by using a scatter plot. You can draw it by using the `ggplot` function, putting the year on the x-axis and the length in words `n` on the y-axis.

```{r message=FALSE, warning=FALSE}
all_words %>%
  group_by(year) %>%
  count() %>%
  ggplot() +
  geom_point(aes(year,
                 n))
```

It seems that for the most part addresses steadily increased in length from 1790 to around 1850, decreased for a few years, and then increased again until the end of the 19th century. The length dramatically decreased around World War I, with a handful of fairly large outliers scattered throughout the 20th century. By 2001 their length decreased and begun a new increase. Is there any rational behind these changes? As we have seen, some presidents delivered written message while other delivered orally. Will it be the reason? To find out, let's colour the points using the `color` argument asking it to colour it by the value of `sotu_type`.

```{r message=FALSE, warning=FALSE}
all_words %>%
  group_by(year) %>%
  count() %>%
  left_join(sotu_meta) %>%
  ggplot() +
  geom_point(aes(year,
                 n,
                 color = sotu_type))
```
As you can see that the rise in the 19th century occurred when the addresses switched to written documents, and the dramatic drop comes when Woodrow Wilson (in 1913) broke tradition and gave his State of the Union as a speech in the Congress. The outliers (the longest one) were all written addresses given after the end of World War II.

Another question we can ask ourselves is whether membership of one party or another has an influence on the length of speeches? We can fiond out changing the argument of `color` from `sotu_type` to `party`.

```{r message=FALSE, warning=FALSE}
all_words %>%
  group_by(year) %>%
  count() %>%
  left_join(sotu_meta) %>%
  ggplot() +
  geom_point(aes(year,
                 n,
                 color = party))
```
It does not seem that the parties have anything to do, but it is curious that since the late 1990s the Democrats are the ones who make the longest speeches. But let's leave these questions, they are more of a student of politics than linguistics.

We saw that texts can be divided into sentences. A feature of style, and of the possible complexity of a text, is the length of sentences. So let's split all speeches into sentences and count how many words.

```{r message=FALSE, warning=FALSE}
all_sentences <- all_sotu %>%
    unnest_tokens(sentence,
                  text,
                  token = "sentences") %>%
  mutate(NumberWords = str_count(sentence,
                            pattern = "\\w+"))
```

Now let's calculate the median and plot it, as we did with the number of words.

```{r message=FALSE, warning=FALSE}
all_sentences %>%
  group_by(year) %>%
  mutate(median = median(NumberWords)) %>%
  left_join(sotu_meta) %>%
  ggplot() +
  geom_point(aes(year,
                 median))
```

The plot shows a strong general trend in shorter sentences over time. Recall that a few addresses in the later half of the 20th century were long, written addresses much like those of the 19th century. It is particularly interesting that these do not show up in terms of the median sentence length. This points out at least one way in which the State of the Union addresses have been changed and adapted over time: shorter sentences.

To make the pattern even more explicit, it is possible to add a smoothing line over the plot with the  `geom_smooth` function. Smoothing lines are a great addition to many plots. They have a dual purpose of picking out the general trend of time series data, while also highlighting any outlying data points.

```{r message=FALSE, warning=FALSE}
all_sentences %>%
    group_by(year) %>%
    mutate(median = median(NumberWords)) %>%
    left_join(sotu_meta) %>%
    ggplot(aes(x = year,
               y = median)) +
    geom_point() +
    geom_smooth()
```

A warning! It is true that there is a general tendency to shorten the length of sentences in speeches, but there is a small problem that I have not taken into account and, although it would not change the final results, it must be taken into account. The tokenizer consider as sentence limits periods, and question and admirations marks, and within these texts there are quite a few abbreviations as mr, as you can see en the following table.

```{r message=FALSE, warning=FALSE}
all_words %>%
  filter(str_detect(word, regex("\\bmr\\b", ignore_case = TRUE))) %>%
  select(word)
```

## Most characteristic words of every president (Keywords){-}

We have seen the most used bigrams in every president speech. They are very illustrative, but they are not the most characteristic words of each of the presidents. Let’s split the texts of all speeches by president, not the individual speeches. In the table below you can see that Roosevelt's most frequent word is “the” and his second most frequent word is “of”. They are the usual common and meaningless words.
```{r message=FALSE, warning=FALSE}
all_words_by_president <- all_sotu %>%
  unnest_tokens(word, text) %>%
  count(president, word, sort = T)

# inspect data (top 10 sorted)

all_words_by_president
```

There is a way to find the most important words in every set (series of speeches for each president) of speeches by comparing between them. The maths behind are complicated, but that's one of the beauties of R and all the libraries that are available. Ono of the methods to find out the most important or characteristic words of every president (or any other text) is know the Term Frequency–Inverse Document Frequency (tf-idf), a statistical measure of keyness which reflects how characteristic a word is of a specific text. Term Frequency–Inverse Document Frequency is based on the frequencies of words in a text compared to the frequency of documents in which it occurs. To find these words it is very easy with the `bind_tf_idf` function. 

```{r eval = FALSE, message=FALSE, warning=FALSE}
all_words_by_president %>%
  bind_tf_idf(word, president, n) %>%
  mutate(president = factor(president,
                            levels = c("George Washington", "John Adams", "Thomas Jefferson",
                                       "James Madison", "James Monroe", "John Quincy Adams",
                                       "Andrew Jackson", "Martin Van Buren", "John Tyler",
                                       "James K. Polk", "Zachary Taylor", "Millard Fillmore",
                                       "Franklin Pierce", "James Buchanan", "Abraham Lincoln",
                                       "Andrew Johnson", "Ulysses S. Grant", "Rutherford B. Hayes",
                                       "Chester A. Arthur", "Grover Cleveland", "Benjamin Harrison",
                                       "William McKinley", "Theodore Roosevelt", "William Howard Taft",
                                       "Woodrow Wilson", "Warren G. Harding", "Calvin Coolidge",
                                       "Herbert Hoover", "Franklin D. Roosevelt", "Harry S Truman",
                                       "Dwight D. Eisenhower", "John F. Kennedy", "Lyndon B. Johnson",
                                       "Richard M. Nixon", "Gerald R. Ford", "Jimmy Carter",
                                       "Ronald Reagan", "George Bush", "William J. Clinton",
                                       "George W. Bush", "Barack Obama", "Donald Trump", "Joe Biden"))) %>%
  group_by(president) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 4, scales = "free")
```
```{r echo=F, fig.cap="", message=FALSE, warning=FALSE, fig.align="center", out.width='10%'}
knitr::include_graphics("https://raw.githubusercontent.com/7PartidasDigital/MLex/master/image/MLex_010.png")
```

There're too many figures, they might be of insterest, but they are meaningless for us. So let’s remove them. To do it we have to redo some work. We have to split in tokens al the speeches, but before doing so, we have to delete all digits with the `mutate` function.

```{r message=FALSE, warning=FALSE}
all_words_by_president <- all_sotu %>%
  mutate(text = str_remove_all(text, "[:digit:]")) %>%
  unnest_tokens(word, text) %>%
  count(president, word, sort = T)
```

Once done, we can plot it again. Nothing changes here.

```{r eval = FALSE, message=FALSE, warning=FALSE}
all_words_by_president %>%
  bind_tf_idf(word, president, n) %>%
  mutate(president = factor(president,
                            levels = c("George Washington", "John Adams", "Thomas Jefferson",
                                       "James Madison", "James Monroe", "John Quincy Adams",
                                       "Andrew Jackson", "Martin Van Buren", "John Tyler",
                                       "James K. Polk", "Zachary Taylor", "Millard Fillmore",
                                       "Franklin Pierce", "James Buchanan", "Abraham Lincoln",
                                       "Andrew Johnson", "Ulysses S. Grant", "Rutherford B. Hayes",
                                       "Chester A. Arthur", "Grover Cleveland", "Benjamin Harrison",
                                       "William McKinley", "Theodore Roosevelt", "William Howard Taft",
                                       "Woodrow Wilson", "Warren G. Harding", "Calvin Coolidge",
                                       "Herbert Hoover", "Franklin D. Roosevelt", "Harry S Truman",
                                       "Dwight D. Eisenhower", "John F. Kennedy", "Lyndon B. Johnson",
                                       "Richard M. Nixon", "Gerald R. Ford", "Jimmy Carter",
                                       "Ronald Reagan", "George Bush", "William J. Clinton",
                                       "George W. Bush", "Barack Obama", "Donald Trump", "Joe Biden"))) %>%
  group_by(president) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 4, scales = "free")
```
```{r echo=F, fig.cap="", message=FALSE, warning=FALSE, fig.align="center", out.width='10%'}
knitr::include_graphics("https://raw.githubusercontent.com/7PartidasDigital/MLex/master/image/MLex_011.png")
```
Now the charts are more meaningful, and it is up to you to interpret them. But there’re some problems we should be aware. If you pay attention to Kennedy's chart you’ll see “viet” and “nam”, but it is one word “viet-nam”. The very same happens with Bush Jr.’s “al-Qaida”, it¸s splited into two segments “al” and “qaida”. So when using these techniques to mine texts we must be aware of the hidden problems texts can contain.

## Collocations{-}

According to Altenberg (1991: 128), “roughly 70% of the running words in the corpus form part of recurrent word combinations of some kind.” The investigation of such word combinations in corpora of authentic language dates back to the earliest studies of collocations by Firth (1957) who summarised this principle as “you shall know a word by the company it keeps”.

In this section we will see the collocations offered by all SOTU speeches. We have seen that a text can be divided into individual words (tokens) but also into n-grams (secuences of n-words or n-tokens). Let's split all SOTU speeches into bigrams.

```{r message=FALSE, warning=FALSE}
all_bigrams <- all_sotu %>%
  unnest_tokens(bigram,
                text,
                token = "ngrams",
                n = 2)

# inspect data (top 10 sorted)

all_bigrams %>%
  count(bigram, sort = T)
```

As usual, the result is not very informative. Again grammatical words, prepositions, determiners and conjunctions float to the top of the table (the exception, expected, is “united states”). We might be tempted to use the `anti_join()` function to erase empty words (function words). However, we can not do it directly since in the list of stopwords that is loaded (`stop_words`) there are only unigrams, there are no bigrams, so the system would not work.

The solution is to separate the bigrams constituents and delete all stopwords. Although you can put everything in one chuck of code, I will present it to you in several steps so that you can see the procedure and its logic.

First, you will separate the two elements with the `separate()` function, and keep each bit in two different columns, `word1` and `word2`.

```{r message=FALSE, warning=FALSE}
separated_bigrams <- all_bigrams %>%
  separate(bigram,
            c("word1", "word2"),
            sep = " ")

# inspect data (top 10 sorted)

separated_bigrams %>%
  count(word1, word2, sort = T)
```

It looks identical to the table that was printed when we splited the texts into bigrams, but now each of the two parts is in a different variable.

The next step is to delete all stopwords, but we will not make use of the `anti_join` function. You are going to extract with `%in%` all stopwords, both in the column `word1` and `word2`, that are not –`!`– in the column `word` of the object `stop_words`.

```{r message=FALSE, warning=FALSE}
filtered_bigrams <- separated_bigrams %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

# inspect data (top 10 sorted)

filtered_bigrams %>%
  count(word1, word2, sort = T)
```

But what you¸re looking at aren’t bigrams, they’re two contiguous words hold in two different columns. We have to reconstruct the bigrams and we will achieve it with the `unite()` function, which brings together into a single column the values contained in two or more columns.

```{r message=FALSE, warning=FALSE}
united_bigrams <- filtered_bigrams %>%
  unite(bigram, word1, word2, sep = " ")

# inspect data (top 10 sorted)

united_bigrams %>%
  count(bigram, sort = T)
```

Now we will plot a graph with the bigrams that each president used most frequently. (It will take a bit of time to plot it.)

```{r eval= FALSE, message=FALSE, warning=FALSE}
united_bigrams %>%
  mutate(president = factor(president,
                            levels = c("George Washington", "John Adams", "Thomas Jefferson",
                                       "James Madison", "James Monroe", "John Quincy Adams",
                                       "Andrew Jackson", "Martin Van Buren", "John Tyler",
                                       "James K. Polk", "Zachary Taylor", "Millard Fillmore",
                                       "Franklin Pierce", "James Buchanan", "Abraham Lincoln",
                                       "Andrew Johnson", "Ulysses S. Grant", "Rutherford B. Hayes",
                                       "Chester A. Arthur", "Grover Cleveland", "Benjamin Harrison",
                                       "William McKinley", "Theodore Roosevelt", "William Howard Taft",
                                       "Woodrow Wilson", "Warren G. Harding", "Calvin Coolidge",
                                       "Herbert Hoover", "Franklin D. Roosevelt", "Harry S Truman",
                                       "Dwight D. Eisenhower", "John F. Kennedy", "Lyndon B. Johnson",
                                       "Richard M. Nixon", "Gerald R. Ford", "Jimmy Carter",
                                       "Ronald Reagan", "George Bush", "William J. Clinton",
                                       "George W. Bush", "Barack Obama", "Donald Trump", "Joe Biden")))
  count(president, bigram, sort = T) %>%
  group_by(president) %>%
  top_n(5) %>%
  ggplot() +
  geom_col(aes(y = n , x = reorder(bigram,n)),
           fill = "maroon") +
  coord_flip() +
  facet_wrap(~ president, ncol = 4, scales = "free")
```

```{r echo=F, fig.cap="", message=FALSE, warning=FALSE, fig.align="center", out.width='10%'}
knitr::include_graphics("https://raw.githubusercontent.com/7PartidasDigital/MLex/master/image/MLex_009.png")
```

There’re many other mining possibilities, as the Part-of-Speech (PoS) tagging which identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word. But we haven’t enough time. I hope this short tutorial serves to awaken your curiosity and you will dare to get the most out of this programming language.

<div class="warning" style='padding:0.1em; background-color:#008080; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>REMEMBER!</b></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<div>
<center>
<img src="https://raw.githubusercontent.com/7PartidasDigital/MLex/master/image/R_google.png" width="500"/>
<center>
</div>
```{r message=FALSE, warning=FALSE}
sessionInfo()
```

<div class="warning" style='padding:0.1em; background-color:#804000; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>NOTE</b>
<br>This R Notebook is based on some [Ladal](https://ladal.edu.au/tutorials.html), [Programming Historian](https://programminghistorian.org/en/lessons/basic-text-processing-in-r) and [CuentaPalabras](http://www.aic.uva.es/cuentapalabras/) tutorials.<br>For linguistics I highly recommend Ladal tutorials.</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<div>
<center>
<img src="https://raw.githubusercontent.com/7PartidasDigital/MLex/master/image/EXPLICIT-7PARTIDAS.png"/>
<br>PID2020-112621GB-I00/AEI/10.13039/501100011033
</center>
</div>
